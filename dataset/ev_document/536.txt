0	Mining New Word Translations from Comparable Corpora
0	New words such as names, technical terms, etc appear frequently.
0	As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.
0	Comparable corpora such as news documents of the same period from different news agencies are readily available.
0	In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.
0	We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.
0	New words such as person names, organization names, technical terms, etc. appear frequently.
0	In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.
0	Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).
0	But parallel corpora are scarce resources, especially for uncommon lan guage pairs.
0	Comparable corpora refer to texts that are not direct translation but are about the same topic.
0	For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.
0	Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.
0	Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).
0	When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.
0	For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation.
0	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.
0	In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.
0	Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.
0	We fully implemented our method and tested it on ChineseEnglish comparable corpora.
0	We translated Chinese words into English.
0	That is, Chinese is the source language and English is the target language.
0	We achieved encouraging results.
0	While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.
0	The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar.
0	We could view this as a document retrieval problem.
0	The context (i.e., the surrounding words) of c is viewed as a query.
0	The context of each candidate translation e' is viewed as a document.
0	Since the context of the correct translation e is similar to e , is considered as a document in IR.
0	If an English word e is the translation of a Chinese word c , they will have similar contexts.
0	So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.
0	The English word e document.
0	We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c . C (e* ) is the this retrieval problem.
0	More details are given in Section 3.
0	On the other hand, when we only look at the word w itself, we can rely on the pronunciation of w to locate its translation.
0	We use a variant of Within IR, there is a new approach to document retrieval called the language modeling approach (Ponte & Croft, 98).
0	In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P(Q | D) , (Knight and Graehl, 1998).
0	More details are is estimated.
0	The document with the highest given in Section 4.
0	Each of the two individual methods provides a P(Q | D) is the one that best matches the query.
0	ranked list of candidate words, associating with each candidate a score estimated by the particular method.
0	If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general.
0	Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.
0	To estimate P(Q | D) , we use the approach of (Ng, 2000).
0	We view the document D as a multinomial distribution of terms and assume that words in both lists and finde1 , e2 ,..., ek that ap query Q is generated by this model: pear in top M positions in both lists.
0	We then n!
0	rank these words e1 , e2 ,..., ek according to the P (Q | D ) = ∏ P (t | D ) c t average of their rank positions in the two lists.
0	∏ t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.
0	If no words appear within the top M positions in both lists, then no translation is output.
0	Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.
0	In particular, our experiment was conducted on comparable corpora that are not very closely related and as such, most of the Chinese words have no translations of times term t occurs in the query Q , n = ∑t ct is the total number of terms in query Q . For ranking purpose, the first fraction n!
0	/ ∏t ct ! can be omitted as this part depends on the query only and thus is the same for all the documents.
0	in the English target corpus.
0	In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document.
0	So
0	our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.
0	∏ P(tc tc∈C ( c ) | T (C (e)))q (tc ) For our task, the query is C (c) , the context Term tc is a Chinese word.
0	q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form.
0	Then we sum lingual dictionary.
0	If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping.
0	ble of pinyin, api is the ith sylla li is the English letter sequence estimation.
0	We use backoff and linear interpolation for probability estimation: P(tc | Tc (C (e))) = α ⋅ Pml (tc | Tc (C (e))) + (1 −α ) ⋅ Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem.
0	We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) ∑dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t∈Tc (C ( e )) lables to English letter sequences.
0	To reduce the search space, we limit the number of English letters that each pinyin syllable can map to as 0, where Pml (•) are the maximu m likelihood esti 1, or 2.
0	Also.
0	we do not allow cross mappin gs.
0	mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.
0	α is set to 0.6 in our experiments.
0	must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e | c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus.
0	Another difference is Onaizan and Knight, 2002b).
0	Knight and Graehl (1998) proposed a probabilistic model for machine transliteration.
0	In this model, a word in the target language (i.e., English in our task) is written and pronounced.
0	This pronunciation is converted to source language pronunciation and then to source language word that our method estimates stead of P(c | e) and P(e) .
0	5.1 Resources.
0	P(e | c)directly, in (i.e., Chinese in our task).
0	AlOnaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters.
0	Pinyin is the standard Romanization system of Chinese characters.
0	It is phonetic-based.
0	For transliteration, we estimate P(e | c) as follows: P(e | c) = P(e | pinyin) = ∑ P(e, a | pinyin) a For the Chinese corpus, we used the Linguistic Data Consortium (LDC) Chinese Gigaword Corpus from Jan 1995 to Dec 1995.
0	The corpus of the period Jul to Dec 1995 was used to come up with new Chinese words c for translation into English.
0	The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new, i.e., not occurring from Jan to Jun 1995.
0	Chinese Giga- word corpus consists of news from two agencies: = ∑∏ P(l a a i | pi ) Xinhua News Agency and Central News Agency.
0	As for English corpus, we used the LDC English Gigaword Corpus from Jul to Dec 1995.
0	The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service.
0	To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency them English translation candidate words.
0	For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates.
0	English Service.
0	The size of the English corpus from Jul to Dec The context C(c)of a Chinese word c was col 1995 was about 730M bytes, and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes.
0	We used a ChineseEnglish dictionary which contained about 10,000 entries for translating the words in the context.
0	For the training of transliteration probability, we required a ChineseEnglish name list.
0	We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm.
0	lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used.
0	The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected.
0	The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing.
0	Unlike English, Chinese text is composed of Chinese characters with no demarcation for words.
0	So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004).
0	for each pair of Chinese source word and English translation candidate word.
0	For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c) | C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e | c) (as described in from a half-month period.
0	Then we determined the new Chinese words in each half-month period p. By new Chinese words, we refer to those words that appeared in this period p but not from Jan to Jun 1995 or any other periods that preceded p. Among all these new words, we selected those occurring at least 5 times.
0	These words made up our test set.
0	We call these words Chinese source words.
0	They were the words that we were supposed to find translations from the English corpus.
0	For the English corpus, we performed sentence segmentation and converted each word to its morphological root form and to lower case.
0	We also divided the English corpus into 12 periods, each containing text from a half-month period.
0	For each period, we selected those English words occurring at least 10 times and were not present in the 10,000-word ChineseEnglish dictionary we used and were not stop words.
0	We considered these English words as potential translations of the Chinese source words.
0	We call Section 4), which was used to rank the English candidate words based on transliteration.
1	Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).
0	If no words appear within the top M positions in both ranked lists, then no translation is output.
0	Note that for many Chinese words, only one English word e appeared within the top M positions for both lists.
0	And among those cases where more than one English words appeared within the top M positions for both lists, many were multiple translations of a Chinese word.
0	This happened for example when a Chinese word was a non-English person name.
0	The name could have multiple translations in English.
0	For example, 米 洛西娜 was a Russian name.
0	Mirochina and Miroshina both appeared in top 10 positions of both lists.
0	Both were correct.
0	5.4 Evaluation.
0	We evaluated our method on each of the 12 half- month periods.
0	The results when we set M = 10 are shown in Table 1.
0	We also investigated the effect of varying M . The results are shown in Table 2.
0	Table 1.
0	Accuracy of our system in each period (M = 10) In Table 1, period 1 is Jul 01 – Jul 15, period 2 is Jul 16 – Jul 31, …, period 12 is Dec 16 – Dec 31.
0	#c is the total number of new Chinese source words in the period.
0	#e is the total number of English translation candidates in the period.
0	#o is the total number of output English translations.
0	#Cor is the number of correct English translations output.
0	Prec.
0	is the precision.
0	The correctness of the English translations was manually checked.
0	Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus.
0	We attempted to estimate recall by manually finding the English translations for all the Chinese source words for the two periods Dec 01 – Dec 15 and Dec 16 – Dec 31 in the English part of the corpus.
0	During the whole December period, we only managed to find English translations which were present in the English side of the comparable corpora for 43 Chinese words.
0	So we estimate that English translations are present in the English part of the corpus for Table 2.
0	Precision and recall for different values of M The past research of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) utilized context information alone and was evaluated on different corpora from ours, so it is difficult to directly compare our current results with theirs.
0	Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable.
0	To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.
0	As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus.
0	This list of 43 words is shown in Table 3.
0	8 of the 43 words are translated to English multi-word phrases (denoted as “phrase” in Table 3).
0	Since our method currently only considers unigram English words, we are not able to find translations for these words.
0	But it is not difficult to extend our method to handle this problem.
0	We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases.
0	The translations of 6 of the 43 words are words in the dictionary (denoted as “comm.” in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as “insuff”).
0	Our method is not able to find 43 (329 + 205) × 4499 = 362words in all 12 pe these translations.
0	But this is due to search space riods.
0	And our program finds correct translations for 115 words.
0	So we estimate that recall (for M = 10) is approximately 115 / 362 = 31.8% . pruning.
0	If we are willing to spend more time on searching, then in principle we can find these translations.
0	Table 3.
0	Rank of correct translation for period Dec 01 – Dec 15 and Dec 16 – Dec 31.
0	‘Cont.
0	rank’ is the context rank, ‘Trans.
0	Rank’ is the transliteration rank.
0	‘NA’ means the word cannot be transliterated.
0	‘insuff’ means the correct translation appears less than 10 times in the English part of the comparable corpus.
0	‘comm’ means the correct translation is a word appearing in the dictionary we used or is a stop word.
0	‘phrase’ means the correct translation contains multiple English words.
0	As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.
0	And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.
0	On the other hand, using our method of combining both sources of information and setting M = ∞, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except 巴佐亚,坩埚,普利法) have their correct English translations at rank one position.
0	If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except 叶玛斯,巴佐亚,坩埚,普利法) have their correct English translations at rank one position.
0	Hence, our method of using both sources of information outperforms using either information source alone.
0	As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information.
0	For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation.
0	On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.
0	In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone.
0	Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling.
0	But their similar spelling clue uses the longest common subsequence ratio and works only for cognates (words with a very similar spelling).
0	The work that is most similar to ours is the recent research of (Huang et al., 2004).
0	They attempted to improve named entity translation by combining phonetic and semantic information.
0	Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.
0	It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging.
0	They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination.
0	In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.
0	We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.
0	We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.
0	This research is partially supported by a research grant R252000-125112 from National University of Singapore Academic Research Fund.
