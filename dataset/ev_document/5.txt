0	A Decision Tree of Bigrams is an Accurate Predictor of Word Sense
0	This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.
0	This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.
0	It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.
0	Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs.
0	For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined.
0	For example, suppose bill has the following set of possible meanings: a piece of currency, pending legislation, or a bird jaw.
0	When used in the context of The Senate bill is under consideration, a human reader immediately understands that bill is being used in the legislative sense.
0	However, a computer program attempting to perform the same task faces a difficult problem since it does not have the bene?t of innate common{sense or linguistic knowledge.
0	Rather than attempting to provide computer programs with real{world knowledge comparable to that of humans, natural language processing has turned to corpus{based methods.
0	These approaches use techniques from statistics and machine learning to induce models of language usage from large samples of text.
0	These models are trained to perform particular tasks, usually via supervised learning.
0	This paper describes an approach where a decision tree is learned from some number of sentences where each instance of an ambiguous word has been manually annotated with a sense{tag that denotes the most appropriate sense for that context.
0	Prior to learning, the sense{tagged corpus must be converted into a more regular form suitable for automatic processing.
0	Each sense{tagged occurrence of an ambiguous word is converted into a feature vector, where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process.
0	Given the exibility and complexity of human language, there is potentially an in?nite set of features that could be utilized.
0	However, in corpus{based approaches features usually consist of information that can be readily iden- ti?ed in the text, without relying on extensive external knowledge sources.
0	These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word.
1	The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.
0	The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.
0	We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy.
0	It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution.
0	One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre{ processing requirements.
0	This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.
0	Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison.
0	The experimental data is discussed, and then the empirical results are presented.
0	We close with an analysis of our ?ndings and a discussion of related work.
0	We have developed an approach to word sense disambiguation that represents text entirely in terms of the occurrence of bigrams, which we de?ne to be two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation of Bigram Counts consecutive words that occur in a text.
0	The distributional characteristics of bigrams are fairly consistent across corpora; a majority of them only occur one time.
1	Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen.
0	We explore two alternatives, the power divergence family of goodness of ?t statistics and the Dice Coecient, an information theoretic measure related to point- wise Mutual Information.
0	Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 ? 2 contingency table.
0	The value of n 11 shows how many times the bigram big cat occurs in the corpus.
0	The value of n 12 shows how often bigrams occur where big is the ?rst word and cat is not the second.
0	The counts in n +1 and n 1+ indicate how often words big and cat occur as the ?rst and second words of any bigram in the corpus.
0	The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family.
0	(Cressie and Read, 1984) introduce the power divergence family of goodness of ?t statistics.
1	A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic.
0	These measure the divergence of the observed (n ij ) and expected (m ij ) bigram counts, where m ij is estimated based on the assumption that the component words in the bigram occur together strictly by chance.
0	(Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed data distributions.
0	However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.
0	In light of this, (Pedersen, 1996) presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio.
0	Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.
0	We take the following approach, based on the observation that all tests should assign approximately the same measure of statistical signi?cance when the bi- gram counts in the contingency table do not violate any of the distributional assumptions that underly the goodness of ?t statistics.
0	We perform tests using X 2 , G 2 , and Fisher's exact test for each bigram.
0	If the resulting measures of statistical signi?cance di?er, then the distribution of the bigram counts is causing at least one of the tests to become unreliable.
0	When this occurs we rely upon the value from Fisher's exact test since it makes fewer assumptions about the underlying distribution of data.
0	For the experiments in this paper, we identi?ed the top 100 ranked bigrams that occur more than 5 times in the training corpus associated with a word.
0	There were no cases where rankings produced by G 2 , X 2 , and Fisher's exact test disagreed, which is not altogether surprising given that low frequency bigrams were excluded.
0	Since all of these statistics produced the same rankings, hereafter we make no distinction among them and simply refer to them generically as the power divergence statistic.
0	2.2 Dice Coecient.
0	The Dice Coecient is a descriptive statistic that provides a measure of association among two words in a corpus.
0	It is similar to pointwise Mutual Information, a widely used measure that was ?rst introduced for identifying lexical relationships in (Church and Hanks, 1990).
0	Pointwise Mutual Information can be de?ned as follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ where w 1 and w 2 represent the two words that make up the bigram.
0	Pointwise Mutual Information quanti?es how often two words occur together in a bigram (the numerator) relative to how often they occur overall in the corpus (the denominator).
0	However, there is a curious limitation to pointwise Mutual Information.
0	A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases.
0	Thus, the maximum pointwise Mutual Information in a given corpus will be assigned to bi- grams that occur one time, and whose component words never occur outside that bigram.
0	These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information.
0	The Dice Coecient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- ecient are similar to those of Mutual Information.
0	The relationship between pointwise Mutual Information and the Dice Coefficient is also discussed in (Smadja et al., 1996).
0	We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.
0	This software is written in Perl and is freely available from www.d.umn.edu/~tpederse.
0	Decision trees are among the most widely used machine learning algorithms.
0	They perform a general to speci?c search of a feature space, adding the most informative features to a tree structure as the search proceeds.
0	The objective is to select a minimal set of features that efficiently partitions the feature space into classes of observations and assemble them into a tree.
0	In our case, the observations are manually sense{tagged examples of an ambiguous word in context and the partitions correspond to the di?erent possible senses.
0	Each feature selected during the search process is represented by a node in the learned decision tree.
0	Each node represents a choice point between a number of di?erent possible values for a feature.
0	Learning continues until all the training examples are accounted for by the decision tree.
0	In general, such a tree will be overly speci?c to the training data and not generalize well to new examples.
0	Therefore learning is followed by a pruning step where some nodes are eliminated or reorganized to produce a tree that can generalize to new circumstances.
0	Test instances are disambiguated by ?nding a path through the learned decision tree from the root to a leaf node that corresponds with the observed features.
0	An instance of an ambiguous word is dis- ambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context.
0	We also include three benchmark learning algorithms in this study: the majority classi?er, the decision stump, and the Naive Bayesian classi?er.
0	The majority classi?er assigns the most common sense in the training data to every instance in the test data.
0	A decision stump is a one node decision tree(Holte, 1993) that is created by stopping the decision tree learner after the single most informative feature is added to the tree.
0	The Naive Bayesian classi?er (Duda and Hart, 1973) is based on certain blanket assumptions about the interactions among features in a corpus.
0	There is no search of the feature space performed to build a representative model as is the case with decision trees.
0	Instead, all features are included in the classi- ?er and assumed to be relevant to the task at hand.
0	There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word.
0	It is most often used with a bag of words feature set, where every word in the training sample is represented by a binary feature that indicates whether or not it occurs in the window of context surrounding the ambiguous word.
0	We use the Weka (Witten and Frank, 2000) implementations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classi?er.
0	Weka is written in Java and is freely available from www.cs.waikato.ac.nz/~ml.
0	Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems.
0	Ten teams participated in the supervised learning portion of this event.
0	Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and in (Kilgarri?
0	and Palmer, 2000).
0	We included all 36 tasks from SENSEVAL for which training and test data were provided.
0	Each task requires that the occurrences of a particular word in the test data be disambiguated based on a model learned from the sense{tagged instances in the training data.
0	Some words were used in multiple tasks as di?erent parts of speech.
0	For example, there were two tasks associated with bet, one for its use as a noun and the other as a verb.
0	Thus, there are 36 tasks involving the disambiguation of 29 di?erent words.
0	The words and part of speech associated with each task are shown in Table 1 in column 1.
0	Note that the parts of speech are encoded as n for noun, a for adjective, v for verb, and p for words where the part of speech was not provided.
0	The number of test and training instances for each task are shown in columns 2 and 4.
0	Each instance consists of the sentence in which the ambiguous word occurs as well as one or two surrounding sentences.
0	In general the total context available for each ambiguous word is less than 100 surrounding words.
0	The number of distinct senses in the test data for each task is shown in column 3.
0	The following process is repeated for each task.
0	Capitalization and punctuation are removed from the training and test data.
0	Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coecient.
0	The bigram must have occurred 5 or more times to be included as a feature.
0	This step ?lters out a large number of possible bi- grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process.
0	The training and test data are converted to feature vectors where each feature represents the occurrence of one of the bigrams that belong in the feature set.
0	This representation of the training data is the actual input to the learning algorithms.
0	Decision tree and decision stump learning is performed twice, once using the feature set determined by the power divergence statistic and again using the feature set identi?ed by the Dice Coecient.
0	The majority classi?er simply determines the most frequent sense in the training data and assigns that to all instances in the test data.
0	The Naive Bayesian classi?er is based on a feature set where every word that occurs 5 or more times in the training data is included as a feature.
0	All of these learned models are used to disambiguate the test data.
0	The test data is kept separate until this stage.
0	We employ a ?ne grained scoring method, where a word is counted as correctly disambiguated only when the assigned sense tag exactly matches the true sense tag.
0	No partial credit is assigned for near misses.
0	The accuracy attained by each of the learning algorithms is shown in Table 1.
0	Column 5 reports the accuracy of the majority classifier, columns 6 and 7 show the best and average accuracy reported by the 10 participating SENSEVAL teams.
0	The evaluation at SENSEVAL was based on precision and recall, so we converted those scores to accuracy by taking their product.
0	However, the best precision and recall may have come from different teams, so the best accuracy shown in column 6 may actually be higher than that of any single participating SENSEVAL system.
0	The average accuracy in column 7 is the product of the average precision and recall reported for the participating SENSEVAL teams.
0	Column 8 shows the accuracy of the decision tree using the J48 learning algorithm and the features identified by a power divergence statistic.
0	Column 10 shows the accuracy of the decision tree when the Dice Coefficient selects the features.
0	Columns 9 and 11 show the accuracy of the decision stump based on the power divergence statistic and the Dice Coefficient respectively.
0	Finally, column 13 shows the accuracy of the Naive Bayesian classifier based on a bag of words feature set.
0	The most accurate method is the decision tree based on a feature set determined by the power divergence statistic.
0	The last line of Table 1 shows the win-tie-loss score of the decision tree/power divergence method relative to every other method.
0	A win shows it was more accurate than the method in the column, a loss means it was less accurate, and a tie means it was equally accurate.
0	The decision tree/power divergence method was more accurate than the best reported SENSEVAL results for 19 of the 36 tasks, and more accurate for 30 of the 36 tasks when compared to the average reported accuracy.
0	The decision stumps also fared well, proving to be more accurate than the best SENSEVAL results for 14 of the 36 tasks.
0	In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice Coecient.
0	The power divergence tests prove to be more reliable since they account for all possible events surrounding two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither.
0	The Dice Coefficient is based strictly on the event where w 1 and w 2 occur together in a bigram.
0	There are 6 tasks where the decision tree / power divergence approach is less accurate than the SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, and sanction-p.
0	The most dramatic difference occurred with amaze-v, where the SENSE- VAL average was 92.4% and the decision tree accuracy was 58.6%.
0	However, this was an unusual task where every instance in the test data belonged to a single sense that was a minority sense in the training data.
0	The characteristics of the decision trees and decision stumps learned for each word are shown in Table 2.
0	Column 1 shows the word and part of speech.
0	Columns 2, 3, and 4 are based on the feature set selected by the power divergence statistic while columns 5, 6, and 7 are based on the Dice Coe- cient.
0	Columns 2 and 5 show the node selected to serve as the decision stump.
0	Columns 3 and 6 show the number of leaf nodes in the learned decision tree relative to the number of total nodes.
0	Columns 4 and 7 show the number of bigram features selected Table 1: Experimental Results.
0	This table shows that there is little di?erence in the decision stump nodes selected from feature sets determined by the power divergence statistics versus the Dice Coecient.
0	This is to be expected since the top ranked bigrams for each measure are consistent, and the decision stump node is generally chosen from among those.
0	However, there are di?erences between the feature sets selected by the power divergence statistics and the Dice Coefficient.
0	These are re ected in the different sized trees that are learned based on these feature sets.
0	The number of leaf nodes and the total number of nodes for each learned tree is shown in columns 3 and 6.
0	The number of internal nodes is simply the di?erence between the total nodes and the leaf nodes.
0	Each leaf node represents the end of a path through the decision tree that makes a sense distinction.
0	Since a bigram feature can only appear once in the decision tree, the number of inter- Table 2: Decision Tree and Stump Characteristics power divergence dice coecient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n by accident 8/15 101 by accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n in excess 13/25 104 in excess 11/21 102 oat-n the oat 7/13 13 the oat 7/13 13 giant-n the giants 16/31 103 the giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n in the 1/1 7 in the 1/1 7 promise-n promise of 95/189 100 a promising 49/97 107 sack-n the sack 5/9 31 the sack 5/9 31 scrap-n scrap of 7/13 8 scrap of 7/13 8 shirt-n shirt and 38/75 101 shirt and 55/109 101 amaze-v amazed at 11/21 102 amazed at 11/21 102 bet-v i bet 4/7 10 i bet 4/7 10 bother-v be bothered 19/37 101 be bothered 20/39 106 bury-v buried in 28/55 103 buried in 32/63 103 calculate-v calculated to 5/9 103 calculated to 5/9 103 consume-v on the 4/7 20 on the 4/7 20 derive-v derived from 10/19 104 derived from 10/19 104 oat-v oated on 24/47 80 oated on 24/47 80 invade-v to invade 55/109 107 to invade 66/127 108 promise-v promise to 3/5 100 promise you 5/9 106 sack-v return to 1/1 91 return to 1/1 91 scrap-v of the 1/1 7 of the 1/1 7 seize-v to seize 26/51 104 to seize 57/113 104 brilliant-a a brilliant 26/51 101 a brilliant 42/83 103 oating-a in the 7/13 10 in the 7/13 10 generous-a a generous 57/113 103 a generous 56/111 102 giant-a the giant 2/3 102 a giant 1/1 101 modest-a a modest 14/27 101 a modest 10/19 105 slight-a the slightest 2/3 105 the slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band of 14/27 100 the band 21/41 117 bitter-p a bitter 22/43 54 a bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p his head 90/179 100 his head 81/161 105 nal nodes represents the number of bigram features selected by the decision tree learner.
0	One of our original hypotheses was that accurate decision trees of bigrams will include a relatively small number of features.
0	This was motivated by the success of decision stumps in performing disambiguation based on a single bigram feature.
0	In these experiments, there were no decision trees that used all of the bigram features identi?ed by the ?ltering step, and for many words the decision tree learner went on to eliminate most of the candidate features.
0	This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7.
0	1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Cocient.
0	This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.
0	If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set.
0	suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses.
0	In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the tree using the gain ratio, a measure based on the overall Mutual Information between the bigram and a particular word sense.
0	Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods.
0	A decision tree with 1 leaf node and no internal nodes (1/1) acts as a majority classi?er.
0	A decision tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump.
0	One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies.
0	We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line.
0	While the accuracy of this approach was as good as any previously published results, the learned models were complex and difficult to interpret, in e?ect acting as very accurate black boxes.
0	Our experience has been that variations in learning algorithms are far less signi?cant contributors to disambiguation accuracy than are variations in the feature set.
0	In other words, an informative feature set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features.
0	Therefore, our focus is on developing and discovering feature sets that make distinctions among word senses.
0	Our learning algorithms must not only produce accurate models, but they should also shed new light on the relationships among features and allow us to continue re?ning and understanding our feature sets.
0	We believe that decision trees meet these criteria.
0	A wide range of implementations are available, and they are known to be robust and accurate across a range of domains.
0	Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation.
0	Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)).
0	While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case.
0	Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).
0	In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word.
0	We believe that the approach in this paper is the ?rst time that decision trees based strictly on bigram features have been employed.
0	The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)).
0	Rather than building and traversing a tree to perform disambiguation, a list is employed.
0	In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over{trained.
0	However, we believe that fragmentation also re ects on the feature set used for learning.
0	Ours consists of at most approximately 100 binary features.
0	This results in a relatively small feature space that is not as likely to su?er from fragmentation as are larger spaces.
0	There are a number of immediate extensions to this work.
0	The ?rst is to ease the requirement that bi- grams be made up of two consecutive words.
0	Rather, we will search for bigrams where the component words may be separated by other words in the text.
0	The second is to eliminate the filtering step by which candidate bigrams are selected by a power divergence statistic.
0	Instead, the decision tree learner would consider all possible bigrams.
0	Despite increasing the danger of fragmentation, this is an interesting issue since the bigrams judged most informative by the decision tree learner are not always ranked highly in the ?ltering step.
0	In particular, we will determine if the filtering process ever eliminates bi- grams that could be significant sources of disambiguation information.
0	In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene?t of sense tagged text.
0	We are optimistic that this is viable, since bigram features are easy to identify in raw text.
0	This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.
0	The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words.
0	The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the Oce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota.
0	We would like to thank the SENSEVAL organizers for making the data and results from the 1998 event freely available.
0	The comments of three anonymous reviewers were very helpful in preparing the ?nal version of this paper.
0	A preliminary version of this paper appears in (Pedersen, 2001).
