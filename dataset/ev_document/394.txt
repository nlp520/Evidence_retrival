0	A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC
0	A word in one language can be translated to zero, one, or several words in other languages.
0	Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.
1	We built a fertility hidden Markov model by adding fertility to the hidden Markov model.
0	This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.
0	It is similar in some ways to IBM Model 4, but is much easier to understand.
0	We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.
0	IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003).
0	There are three kinds of important information for word alignment models: lexicality, locality and fertility.
0	IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago.
0	Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.
0	Nevertheless, we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the tar get language.
0	However, IBM Model 4 is so complex that most researches use the GIZA++ software package (Och and Ney, 2003), and IBM Model 4 itself is treated as a black box.
0	The complexity in IBM Model 4 makes it hard to understand and to improve.
0	Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.
0	We also want it to be accurate and computationally efficient.
0	There have been many years of research on word alignment.
0	Our work is different from others in essential ways.
0	Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.
0	Directly modeling fertility makes our model fundamentally different from others.
0	Most models have limited ability to model fertility.
0	Liang et al.
0	(2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1.
0	ITG models (Wu, 1997) assume the fertility to be either zero or one.
0	It can model phrases, but the phrase has to be contiguous.
0	There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly.
0	Our model is a coherent generative model that combines the HMM and IBM Model 4.
0	It is easier to understand than IBM Model 4 (see Section 3).
0	Our model also removes several undesired properties in IBM Model 4.
0	We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, MIT, Massachusetts, USA, 911 October 2010.
0	Qc 2010 Association for Computational Linguistics estimation.
0	Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993).
0	Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable.
0	Our model is much faster than IBM Model 4.
0	In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.
0	Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.
0	Brown et al.
0	(1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility.
0	If the optimal alignment is not in those neighbors, this method will not be able find the opti total of I + 1 empty words for the HMM model1.
0	Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1.
0	After we add I + 1 empty words to the target sentence, the alignment is a mapping from source to target word positions: a : j → i, i = aj where j = 1, 2, . . .
0	, J and i = 1, 2, . . .
0	, 2I + 1.
0	Words from position I + 1 to 2I + 1 in the target sentence are all empty words.
0	We allow each source word to align with exactly one target word, but each target word may align with multiple source words.
0	The fertility φi of a word ei at position i is defined as the number of aligned source words: J mal alignment.
0	We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, φi = j=1 δ(aj , i) which has nice probabilistic guarantees.
0	DeNero et al.
0	(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.
0	where δ is the Kronecker delta function: ( 1 if x = y δ(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility.
0	the target sentence is "£2I +1 "£2I +1 φi. We define φǫ ≡ 2I +1 i=I +1 φi. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . .
0	, fJ and a f J , we have "£I φi + φǫ = J . target sentence eI 1 = e1, e2, . . .
0	, eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions.
0	Following Brown et al.
0	(1993), we assume that each source word is aligned to exactly one target word.
0	get sentence are a set Bi, such that each element in Bi is aligned with i, and all alignments of i are in Bi.
0	Inverted alignments are explicitly used in IBM Models 3, 4 and 5, but not in our model, which is We denote as aJ = a1, a2, . . .
0	, aJ the alignments one reason that our model is easier to understand.
0	between f J and eI . When a word fj is not aligned 1 1 with any word e, aj is 0.
0	For convenience, we add an empty word ǫ to the target sentence at position 0 (i.e., e0 = ǫ).
0	However, as we will see, we have to add more than one empty word for the HMM.
0	2.2 IBM Model 1 and HMM.
0	IBM Model 1 and the HMM are both generative models, and both start by defining the probability of alignments and source sentence given the In order to compute the “jump probability” in the target sentence: P (aJJ 1 ); the data likeli HMM model, we need to know the position of the 1 , f1 |e2I +1 hood can be computed by summing over alignments: aligned target word for the previous source word.
0	If the previous source word aligns to an empty word, 1 If fj.
0	−1 does not align with an empty word and fj alignswe could use the position of the empty word to indi with an empty word, we want to record the position of the target word that fj−1 aligns with.
0	There are I + 1 possibilities: fj is cate the nearest previous source word that does not align to an empty word.
0	For this reason, we use a the first word in the source sentence, or fj the target word.
0	−1 aligns with one ofP (f J |e2I +1) = "£ J P (aJ , f J |e2I +1).
0	The alignwhere the first two equations imply that the proba 1 1 a1 1 1 1 ments aJ are the hidden variables.
0	The expectation maximization algorithm is used to learn the parameters such that the data likelihood is maximized.
0	Without loss of generality, P (aJ , f J |e2I +1) can bility of jumping to an empty word is either 0 or p0, and the third equation implies that the probability of jumping from a nonempty word is the same as the probability of jumping from the corespondent empty 1 1 1 be decomposed into length probabilities, distortion probabilities (also called alignment probabilities), and lexical probabilities (also called translation probabilities): P (aJ , f J |e2I +1) 1 1 1 J word.
0	The absolute position in the HMM is not important, because we re-parametrize the distortion probability in terms of the distance between adjacent alignment points (Vogel et al., 1996; Och and Ney, 2003): = P (J |e2I +1) n P (aj , fj |f j−1, aj−1, e2I +1) c(i − i′) 1 j=1 1 1 1 P (i|i′, I ) = "£ i′′ c(i′′ − i′) J = P (J |e2I +1) n P (aj |f j−1, aj−1, e2I +1) × where c( ) is the count of jumps of a given distance.
0	1 j=1 1 1 1 In IBM Model 1, the word order does not mat ter.
0	The HMM is more likely to align a source P (fj |f j−1, aj , e2I +1)l 1 1 1 where P (J |e2I +1) is a length probability, word to a target word that is adjacent to the previous aligned target word, which is more suitable than IBM Model 1 because adjacent words tend to form (aj |f j−1, aj−1 2I +1P 1 1 , e1 ) is a distortion prob phrases.
0	ability and P (fj |f j probability.
0	−1, aj , e 2I +1 1 ) is a lexical For these two models, in theory, the fertility for a target word can be as large as the length of the IBM Model 1 assumes a uniform distortion probability, a length probability that depends only on the length of the target sentence, and a lexical probability that depends only on the aligned target word: J source sentence.
0	In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can become a garbage collector, and align to many source words (Brown et al., 1993; Och and Ney, 2003; Moore, 2004).
0	The HMM is P (aJ , f J |e2I +1) = P (J |I ) n P (f |e ) less likely to have this garbage collector problem be 1 1 1 (2I + 1)J j=1 j aj cause of the alignment probability constraint.
0	However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.
0	This is our motivation for adding fertility to these two models, and we expect that the resulting models will perform better than the baseline models.
0	Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1.
0	Throughout the paper, “our model” refers to the fertility hidden Markov model.
0	Due to space constraints, we are unable to provide details for IBM Models 3, 4 and 5; see Brown et al.
0	(1993) and Och and Ney (2003).
0	But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4.
0	IBM Model 5 removes deficiency (Brown et al., 1993; Och and Ney, 2003) from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy.
0	Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (φI , φǫ, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability.
0	IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn.
0	Our model has only one parameter for each target word, which can be learned more reliably.
0	In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (φI , φǫ, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I φi 1 λ(ei ) summing over fertilities and alignments: n λ(ei) e− × P (f J |e2I +1) = "£ I J P (φI , φǫ, aJ , f J |e2I +1).
0	i=1 φi! 1 1 φ1 ,φǫ ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable φi, and we assume φi follows a Poisson distribution Poisson(φi; λ(ei)).
0	The sum of the fer (I λ(ǫ))φǫ e−(I λ(ǫ)) φǫ!
0	× J tilities of all the empty words (φǫ) grows with the length of the target sentence.
0	Therefore, we assume that φǫ follows a Poisson distribution with parameter I λ(ǫ).
0	Now P (φI , φǫ, aJ , f J |e2I +1) can be decomposed 1 (2I + 1)J n P (fj | j=1 eaj ) (1) 1 1 1 1 in the following way: P (φI , φǫ, aJ , f J |e2I +1) In the fertility HMM, we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence, and that 1 1 1 1 = P (φI |e2I +1)P (φǫ|φI , e2I +1) × 1 1 1 1 J the lexical probability depends only on the aligned target word: n P (aj , fj |f j−1, aj−1, e2I +1, φI , φǫ) j=1 1 1 1 1 P (φI , φǫ, aJ , f J |e2I +1) = n λ(ei) e−λ(ei ) 1 1 1 I φ 1 λ(e ) φi! × = n λ(ei) i e− i i=1 (I λ(ǫ))φǫ e−I λ(ǫ) φǫ!
0	× φ i=1 (I λ(ǫ))φǫ ! × e−(I λ(ǫ)) J n P (aj |f j−1, aj−1, e2I +1 I φǫ!
0	× J j=1 1 1 1 , φ1 , φǫ) × n P (aj | j=1 aj−1 , I )P (fj | eaj ) (2) P (fj |f j−1, aj , e2I +1, φI , φǫ)l 1 1 1 1 Superficially, we only try to model the length 1 |e2I +1probability more accurately.
0	However, we also en When we compute P (f J 1 ), we only sum force the fertility for the same target word across the corpus to be consistent.
0	The expected fertility for a nonempty word ei is λ(ei), and the expected fertil over fertilities that agree with the alignments: ity for all empty words is I λ(ǫ).
0	Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), λ(e), ξ1(e) , ξ2(a )) 1 1 1 = P (φI , φǫ, aJ , f J |e2I +1) = P˜ ′ aJ e 2I +1, f J ) log ′ P (aJ , f J | e2I +1) 1 1 ,φǫ 1 1 1 1 1 1 J 1 1 1 1 ≈ P (φI , φǫ, aJ , f J |e2I +1) × − ξ1(e)( P (f |e) − 1) 1 1 1 1 I  J  e f n δ  i=1 j=1 δ(aj , i), φi × − ξ2(a′)( a′ a P (a|a′) − 1)  2I +1 J  Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 δ  i=I +1 j=1 δ(aj , i), φǫ (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, φǫ and each P (f |e) = "£s c (f |e; f (s), e(s)) (4) φi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.
0	Because we only sum over fer tilities that are consistent with the alignments, we P (a|a′) = "£s c (a|a′; f (s), e(s)) (5)have "£f J P (f J |e2I +1) < 1, and our model is de "£ "£ a s c(a|a′; f (s), e(s)) 1 1 1 "£ (s) (s) ficient, similar to IBM Models 3 and 4 (Brown et al., 1993).
0	We can remove the deficiency for fertility IBM Model 1 by assuming a different distortion λ(e) = s c(φ| e; f , e ) s c(k|e; f (s), e(s)) (6) probability: the distortion probability is 0 if fertility where s is the number of bilingual sentences, andis not consistent with alignments, and uniform oth c(f |e; f J 2I +1 ˜ J J 2I +1 erwise.
0	The total number of consistent fertility and 1 , e1 ) = P (a1 |f1 , e1 ) × J alignments is J ! .
0	Replacing 1 with a1 φǫ ! J i ! φǫ ! J i !
0	(2I +1)J δ(fj , f )δ(ei, e) J ! , we have: c(a|a′; f J , e2I +1) = j P˜(aJ |f J , e2I +1) × P (φI , φǫ, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n λ(ei)φi e−λ(ei ) × i=1 (I λ(ǫ))φǫ e−(I λ(ǫ)) × c(φ|e; f1 , e1 ) = δ(aj , a)δ(aj−1, a′) j P˜(a1 |f1 , e1 ) × J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 φ δ(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)δ(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency.
0	We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.
0	For the fertility IBM Model 1, we do not need to estimate the distortion probability.
0	Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).
0	The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.
0	We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts.
0	1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.
0	During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, · · · aj−1, aj+1 · · · aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, · · · , aj 1, a , · · · , a , f J , e2I +1) − j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end "£ J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples.
0	We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.
0	This Gibbs sampling method updates parameters constantly, so it is an “online learning” algorithm.
0	However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.
0	Instead, we do “batch learning”: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).
0	This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step.
0	In this case, the fertility hidden Markov model is not faster than the HMM.
0	Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.
0	In the testing stage, the sampling algorithm is the same as above except that we keep the alignments 1 that maximize P (a1 , f1 |e2I +1).
0	We need more the HMM do in the EM algorithms.
0	The algorithm aJ J J 1 for the E-step on one machine (all machines are independent) is in Algorithm 1.
0	For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . .
0	, J ) is O(tI J ).
0	This is the same complexity as the HMM if t is O(I ), and it has lower complexity if t is a constant.
0	Surprisingly, we can achieve better results than the HMM by computing as few as 1 sample for each alignment, so the fertility hidden Markov model is much faster than the HMM.
0	Even when choosing t such that our model is 5 times faster than the HMM, we achieve better results.
0	2 For fertility IBM Model 1, we only need to compute I + 1.
0	values because e2I +1 are identical empty words.
0	samples in the testing stage because it is unlikely to get to the optimal alignments by sampling a few times for each alignment.
0	On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level.
0	Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.
0	Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.
0	Gibbs sampling for the fertility IBM Model 1 is similar but simpler.
0	We omit the details here.
0	Al ig n m en t M o d e l P R A E R e n → c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7.
0	8 4 3.
0	8 3 9.
0	0 3 7.
0	9 3 6.
0	2 3 4.
0	9 3 4.
0	5 c n → e n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 52 .6 55 .9 66 .1 68 .6 71 .1 71 .1 69 .3 53 .7 56 .4 62 .1 60 .2 62 .2 62 .7 68 .5 4 6.
0	9 4 3.
0	9 3 5.
0	9 3 5.
0	7 3 3.
0	5 3 3.
0	2 3 1.
0	1 Table 1: AER results.
0	IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.
0	We choose t = 1, 5, and 30 for the fertility HMM.
0	0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 1: AER comparison (en→cn) 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 2: AER comparison (cn →en) 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3: Training time comparison.
0	The training time for each model is calculated from scratch.
0	For example, the training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3.
0	We evaluated our model by computing the word alignment and machine translation quality.
0	We use the alignment error rate (AER) as the word alignment evaluation criterion.
0	Let A be the alignments output by word alignment system, P be a set of possible alignments, and S be a set of sure alignments both labeled by human beings.
0	S is a subset of P . Precision, recall, and AER are defined as follows: recall = |A ∩ S| |S| precision = |A ∩ P | |A| AER(S, P, A) = 1 |A ∩ S| + |A ∩ P | |A| + |S| AER is an extension to F-score.
0	Lower AER is better.
0	We evaluate our fertility models on a ChineseEnglish corpus.
0	The ChineseEnglish data taken from FBIS newswire data, and has 380K sentence pairs, and we use the first 100K sentence pairs as our training data.
0	We used hand-aligned data as reference.
0	The ChineseEnglish data has 491 sentence pairs.
0	We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.
0	We smooth all parameters (λ(e) and P (f |e)) by adding a small value (10−8), so they never become too small.
0	We run both models for 5 iterations.
0	AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.
0	We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.
0	We smooth all parameters (λ(e), P (a|a′) and P (f |e)) by adding a small value (10−8).
0	We run both models for 5 iterations.
0	AER results are computed using traditional HMM Viterbi decoding for both models.
0	It is always difficult to determine how many samples are enough for sampling algorithms.
0	However, both fertility models achieve better results than their baseline models using a small amount of samples.
0	For the fertility IBM Model 1, we sample 10 times for each aj , and restart 3 times in the training stage; we sample 100 times and restart 12 times in the testing stage.
0	For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.
0	More samples give no further improvement.
0	Initially, the fertility IBM Model 1 and fertility HMM did not perform well.
0	If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter λ(e).
0	Hence, smoothing is needed.
0	One may try to solve it by forcing all these words to share a same parameter λ(einfrequent).
0	Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.
0	We solve the problem in the following way: estimate the parameter λ(enon empty ) for all nonempty words, all infrequent words share this parameter.
0	We consider words that appear less than 10 times as infrequent words.
0	Table 1, Figure 1, and Figure 2 shows the AER results for different models.
0	We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.
0	The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.
0	Figure 3 show the training time for different models.
0	In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM.
0	It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase).
0	We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model.
0	We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007).
0	The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic.
0	Our test is 633 sentences of up to length 50, with four references.
0	Results are shown in Table 2; we see that better word alignment results do not lead to better translations.
0	Model BLEU HMM 19.55 HMMF30 19.26 IBM4 18.77 Table 2: BLEU results
0	We developed a fertility hidden Markov model that runs faster and has lower AER than the HMM.
0	Our model is thus much faster than IBM Model 4.
0	Our model is also easier to understand than IBM Model 4.
0	The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.
0	While better word alignment results do not necessarily correspond to better translation quality, our translation results are comparable in translation quality to both the HMM and IBM Model 4.
0	Acknowledgments We would like to thank Tagyoung Chung, Matt Post, and the anonymous reviewers for helpful comments.
0	This work was supported by NSF grants IIS0546554 and IIS0910611.
